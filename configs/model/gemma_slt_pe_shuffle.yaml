type:
  _target_: model.gemma_slt.Gemma3SLT

mname: google/gemma-3-12b-pt

backbone:
  _target_: model.gemma_slt.PerceptionEncoderBackbone
  id: PE-Lang-L14-448

visual_adapter:
  _target_: model.gemma_slt.PEAdapter
  input_hidden_size: 768
  output_hidden_size: 2560
  scale_factor: 4
  pooling_num_probe: 1
  pooling_num_heads: 4
  pooling_mlp_ratio: 4

visual_encoder_layer_scale: 1
# connector_depth: 1
#
lora_config:
  r: 4
  lora_alpha: 16
  lora_dropout: 0.025
  bias: none

chat_template: jinjas/gemma_slt.jinja
random_video_mask: 0.0
