optimizer:
  _target_: torch.optim.Adam
  lr: 1e-5

lr_scheduler:
  type: native
  instance:
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 50
  # name: linear
  # warmup_steps: 0
  # training_steps: 100
  # kwargs: null
