optimizer:
  _target_: torch.optim.Adam
  lr: 1e-5

lr_scheduler:
  type: native
  instance:
    _target_: torch.optim.lr_scheduler.MultiStepLR
    milestones: [50, 75, 90]
    gamma: 0.1
  # name: linear
  # warmup_steps: 0
  # training_steps: 100
  # kwargs: null
